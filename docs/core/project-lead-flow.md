# Project Lead Flow

**Last Updated:** 2026-02-20  
**Purpose:** Complete specification of Project Lead orchestration across all modes and phases.  
**Audience:** Used as reference when building/updating Project Lead AGENTS.md.

**Recent Updates:**
- v5.1 (2026-02-20): **SALLY MOCKUPS MANDATORY.** design-assets.json is no longer optional. Sally MUST use the `frontend-design` skill to build HTML prototypes + Playwright screenshots for every main screen after writing ux-design.md. PL task prompt now explicitly requires this with a pointer to `docs/core/design-workflow.md`. Removed stale "Figma URLs" reference from Sally step.
- v5.0 (2026-02-20): **DEFAULT TECH STACK.** `docs/core/tech-stack.md` created. Winston must read it before writing architecture.md on every project. Default: Next.js 15 + TypeScript + Tailwind + shadcn/ui + pnpm + Supabase (DB+Auth) + Drizzle + Vercel. Override by specifying in intake.md; document as ADR in architecture.md.
- v4.9 (2026-02-20): **NO MOCKING IN E2E â€” REAL EVERYTHING.** Phase 3 E2E tests must use real deployed URL, real Firebase Auth, real credentials. No stubbing `signInWithPopup` or any external service. Console error listener mandatory in every test file (CSP violations = auto-fail). Test credentials must be a dedicated factory test Gmail (NOT kelly@bloomtech.com). If auth project and `test-credentials.md` missing â†’ Murat halts + PL notifies Kelly. Same policy codified in murat-testing/SKILL.md.
- v4.8 (2026-02-19): **FULL QA FEEDBACK WORKFLOWS IN ALL AGENTS.** John: `scope-qa-feedback` (4th responsibility). Winston: `review-architecture-change` (2nd responsibility). Amelia: `fix-qa-feedback` (Mode 3). Bob: `update-dependency-graph` (4th responsibility). Every agent now knows its QA feedback role â€” PL routes, agents execute.
- v4.6 (2026-02-19): **PHASE 3 HARD GATE + FORBIDDEN STORY TYPES.** Phase 3 (TEA) is now an explicit hard gate â€” cannot be skipped, cannot be substituted by a Phase 2 "Smoke Test" story. John AGENTS.md: forbidden from creating testing/deployment epics. Bob AGENTS.md: must skip any test/deploy stories that slip through from John's epics. Root cause: Verdict shipped with zero test artifacts because story 7.2 "Production Build, Deploy & Smoke Test" was mistaken for Phase 3.
- v4.1 (2026-02-19): **STATELESS PL + CONTEXT DISCIPLINE.** PL must keep replies to 1-2 lines, never narrate history, rotate session every 25 stories. Prevents 200k token overflow on large projects. See Context Discipline section.
- v4.0 (2026-02-19): **DESIGN WORKFLOW INTEGRATION.** Sally outputs design-assets.json with Figma URLs, Bob adds design_references to stories, Amelia uses Figma MCP + frontend-design skill for visual fidelity. See [design-workflow.md](./design-workflow.md) for full details. âœ… Implemented (Figma MCP configured, all agents updated)
- v4.5 (2026-02-19): **QA FEEDBACK = STORY FLOW.** QA feedback from the operator creates new stories and updates BMAD artifacts â€” same pipeline as greenfield Phase 2. Bug exception only: if feedback is a missed implementation or something clearly broken (not new behavior), Amelia fixes directly without story creation. No qaRounds[] in registry â€” feedback is tracked in BMAD artifacts like everything else.
- v4.4 (2026-02-19): **FAST MODE REMOVED.** Factory runs Normal Mode only: Greenfield or Brownfield. Barry Fast Track eliminated.
- v4.3 (2026-02-19): **PENDING-QA STATE + PL HOLDS SESSION.** After Phase 3 TEA passes, PL sets `state: "pending-qa"` and enters an idle hold â€” the session stays alive (lock file held) until Kelly signals SHIP, FIX, or PAUSE. PL MUST NOT exit or mark shipped on its own. Only the operator (via Kelly) can trigger ship. Dashboard shows the live PL session as "AWAITING QA".
- v4.2 (2026-02-19): **PHASE NAMING + TEA STREAMLINED.** Phase 3 renamed "Test" (was "Post-Deploy Verification/QA"). TEA simplified: TD+TF+TA combined into single Murat "test-generate" pass. Removed RV (test review) and TR (traceability) â€” redundant overhead for MVP factory. New TEA: test-generate â†’ E2E execution + NR in parallel.
- v3.3 (2026-02-19): **CODE REVIEW DISABLED.** Stories now go dev â†’ done directly (skipping code-review Amelia). Rationale: 80%+ reviews pass, adds 5-10 min overhead per story, Phase 3 TEA testing more thorough. Can re-enable once factory proven.
- v3.2 (2026-02-19): Restructured Phase 3 into Pre-Deploy Gates â†’ Deploy â†’ Post-Deploy Verification. Full TEA suite (TD, TF, TA, RV, TR, NR) runs against deployed app. Failures batched â†’ Amelia remediates â†’ redeploy â†’ re-run. Removed correct-course routing for QA failures (direct to Amelia).
- v3.1 (2026-02-18): Automated E2E test generation via Murat trace + automate workflows.

---

## Overview

Project Lead owns a single project from intake to ship. One PL session per project. PL spawns BMAD agents as subagents and tracks their progress.

**Intake Source:** Research Lead creates comprehensive intake document at `projects/ideas/<project-id>/intake.md`. When starting a new project, Project Lead reads the registry entry's `researchDir` field to locate the intake file and supporting research documents (solution scoring, competitive analysis, naming options).

**State tracking:** PL updates `projects/projects-registry.json` at key lifecycle transitions. See `docs/core/project-registry-workflow.md` for full spec.

**Story status:** BMAD artifacts (`sprint-status.yaml`, `dependency-graph.json`) track implementation progress.

**Registry updates (PL responsibility):**
- **Project start:** `discovery` â†’ `in-progress` (set `implementation.projectDir`, `timeline.startedAt`)
- **QA ready:** `in-progress` â†’ `pending-qa` (set `implementation.qaUrl`, `timeline.lastUpdated`, notify Kelly)
- **Followup:** `shipped` â†’ `followup` (add entries to `followup[]`)
- **Followup done:** `followup` â†’ `shipped`
- **Pause/Resume:** Set `paused: true/false` with `pausedReason`

**Registry updates (PL does on Kelly's SHIP signal â€” operator approval required first):**
- **Ship:** `pending-qa` â†’ `shipped` (set `implementation.deployedUrl`, `timeline.shippedAt`) â€” only after receiving `"SHIP: {projectId}"` from Kelly

**Dependency authority:** Bob's `dependency-graph.json` (or `stories-parallelization.json`) in `_bmad-output/implementation-artifacts/`. Each story has individual `dependsOn` arrays.

---

## CLI-First Policy

**All planning artifacts (architecture, stories) must specify CLI commands, not browser steps.**

- Winston writes: `gcloud projects create "$PROJECT_ID"` (not "Navigate to Firebase Console")
- Bob writes: `firebase apps:create web` (not "Click Add App button")
- Amelia executes: CLI tools first, browser only if no CLI exists

**Lightest rule:** CLI-first. Browser only if no CLI exists.

---

## âš ï¸ Vercel Deploy Limit (Free Tier)

**Vercel free plan: 100 deployments/day hard limit.**

When a project uses Vercel:
1. **Disable git auto-deploy immediately.** Every `git push` from Amelia's stories triggers a deploy. With 20-80 stories each pushing commits, you'll blow through 100 in hours.
   - Set via Vercel dashboard â†’ Project Settings â†’ Git â†’ disable "Deploy on Push"
   - Or: don't link the git repo to Vercel at all. Use CLI-only deploys.
2. **Zero deploys during Phase 2.** Amelia must NEVER run `vercel` CLI or push to a Vercel-connected branch during the build phase. Stories are `git push`-only to `dev`.
3. **Single deploy in Phase 3 Step 2.** One intentional `vercel --prod` at the start of Phase 3. That's it.
4. **If limit is hit:** Fall back to Firebase Hosting (`firebase deploy --only hosting`) for QA. Re-deploy to Vercel when limit resets (24h rolling window).

---

## Normal Mode Greenfield

### Phase 0: Git Init (MANDATORY â€” do this first, before anything else)

Every project has its own git repo. The `clawd` workspace ignores `projects/*/` in its `.gitignore`, so project code never bleeds into clawd's history.

```bash
# Run this immediately after the project directory is created:
cd /Users/austenallred/clawd/projects/{projectId}
git init
git add -A
git commit -m "feat: initial project setup â€” {ProjectName}"
```

- **Default branch:** `main`
- **No remote needed yet** â€” local history only until ship time
- **On ship:** create private GitHub repo + push: `gh repo create austenallred/{projectId} --private --source=. --push`
- **All repos must be private** â€” never create a public repo for factory projects

### Phase 1: Plan

All sequential â€” each step waits for the previous to complete.

**CRITICAL: All BMAD spawns MUST include YOLO MODE directive.** Without it, workflows halt at confirmation prompts and subagents time out waiting for input that never comes.

```
1. John: create-prd
   â†’ Input: intake.md
   â†’ Output: _bmad-output/planning-artifacts/prd.md
   â†’ Task MUST include: "YOLO MODE â€” skip all confirmations, run fully autonomously."

2. Sally: create-ux-design
   â†’ Input: prd.md
   â†’ Output: _bmad-output/planning-artifacts/ux-design.md
   â†’ **MANDATORY:** _bmad-output/design-assets.json + _bmad-output/design-assets/screens/*.html + _bmad-output/design-assets/images/screens/*.png
   â†’ Task MUST include: "YOLO MODE â€” skip all confirmations, run fully autonomously."
   â†’ Task MUST include explicit instruction: "After writing ux-design.md, read and follow docs/core/design-workflow.md in full. Use the frontend-design skill to build a self-contained HTML prototype for each main screen, screenshot each one with Playwright, and write design-assets.json with the screens record pointing to the PNG paths. This is NOT optional."
   â†’ After Sally completes: read _bmad-output/design-assets.json and set projectState.designAssets (dashboard displays screen mockups)

3. Winston: create-architecture
   â†’ Input: prd.md, ux-design.md
   â†’ Output: _bmad-output/planning-artifacts/architecture.md
   â†’ Task MUST include: "YOLO MODE â€” skip all confirmations, run fully autonomously."
   â†’ **MANDATORY:** Winston must read `docs/core/tech-stack.md` before writing architecture.md. Use the default stack unless intake.md explicitly overrides a layer. Document any deviation as an ADR in architecture.md.

4. John: create-epics-and-stories (SEPARATE from create-prd)
   â†’ Input: prd.md, architecture.md, ux-design.md
   â†’ Output: _bmad-output/planning-artifacts/epics.md
   â†’ Task MUST include: "YOLO MODE â€” skip all confirmations, run fully autonomously."

5. John: check-implementation-readiness (GATE CHECK)
   â†’ Input: prd.md, epics.md, architecture.md
   â†’ Output: PASS / CONCERNS / FAIL / NEEDS WORK / NOT READY (all treated as PASS or NOT PASS)
   â†’ Task MUST include: "YOLO MODE â€” skip all confirmations, run fully autonomously."
   
   **GATE LOGIC (STRICT):**
   
   **PASS / READY** â†’ Proceed to Bob (step 6)
   
   **NOT PASS** (CONCERNS / FAIL / NEEDS WORK / NOT READY) â†’ Remediation Loop
   - Do NOT proceed to Bob until gate check returns PASS
   - ANY documented concerns require fixes before the build phase
   - Prevents shipping with known issues or technical debt
   
   **Remediation Loop (for NOT PASS):**
   
   1. **Project Lead reads gate check report** (`implementation-readiness-check.md`)
      - Identify all documented issues (IMMEDIATE, HIGH PRIORITY, MEDIUM PRIORITY)
      - Categorize by artifact: PRD, UX, Architecture, Epics
   
   2. **Route to appropriate persona(s) for fixes:**
      - **PRD gaps/issues** â†’ John (edit-prd)
        - Missing requirements, unclear scope, stakeholder decisions needed
      - **UX issues** â†’ Sally (edit-ux-design)
        - Missing screens, incomplete user flows, accessibility gaps
      - **Architecture gaps** â†’ Winston (edit-architecture)
        - Technical feasibility concerns, missing ADRs, infrastructure gaps
      - **Epic/story issues** â†’ John (edit epics.md directly OR create new stories via Bob)
        - Missing epics, incomplete stories, missing acceptance criteria
        - Option A: John edits epics.md to add missing content
        - Option B: Bob creates new story files for missing functionality (if epics complete but stories missing)
   
   3. **Spawn persona(s) with specific fix instructions from report:**
      - Pass exact issues from gate check report to spawned agent
      - Example: "Fix IMMEDIATE concerns #1-5 from implementation-readiness-check.md"
   
   4. **Re-run John: check-implementation-readiness**
      - After fixes applied, spawn John again with same workflow
      - John validates fixes and produces updated report
   
   5. **Repeat until PASS**
      - Maximum 3 remediation cycles (escalate to operator if stuck)
      - Track remediation attempts in project-state.json

6. Bob: sprint-planning
   â†’ Input: epics.md
   â†’ Output: _bmad-output/implementation-artifacts/sprint-status.yaml

7. Bob: Create dependency-graph.json (CUSTOM FACTORY LOGIC, not BMAD)
   â†’ Input: epics.md, architecture.md
   â†’ Output: _bmad-output/implementation-artifacts/dependency-graph.json

8. Bob: create-story (ONE spawn â€” writes ALL stories sequentially in one session)
   â†’ Spawn Bob ONCE with /bmad-bmm-create-story; Bob reads epics.md and writes every story file in sequence
   â†’ Input: epics.md, architecture.md, prd.md, ux-design.md, design-assets.json (if exists)
   â†’ Output: _bmad-output/implementation-artifacts/stories/story-{N.M}.md (all stories)
   â†’ Stories include design_references field when design-assets.json exists (see [design-workflow.md](./design-workflow.md))
   â†’ âš¡ One session = ~15 min regardless of story count (vs one spawn per story = 90+ min for 30 stories)
```

### Phase 2: Implement â€” Dependency-Driven Parallelization

**ðŸš« NO VERCEL DEPLOYS IN PHASE 2.** Amelia's stories `git push` to dev only. If Vercel git auto-deploy is enabled, every push burns a daily deploy slot. Disable it before Phase 2 starts (see Vercel Deploy Limit section above).

**ðŸ“‹ Dev server URL.** After the first setup story completes, Amelia reports the local dev server URL (e.g. `http://localhost:5173`). PL must immediately write it to `project-state.json`:
```bash
# as soon as Amelia's first story finishes and the dev server is running:
python3 -c "
import json
f = 'project-state.json'
d = json.load(open(f))
d['devServerUrl'] = 'http://localhost:PORT'  # replace PORT with actual port
json.dump(d, open(f,'w'), indent=2)
"
```
This shows up as the **Local Dev** link on the Kelly Dashboard project detail page.

**No artificial batching or waves.** Spawn stories as soon as their dependencies are satisfied.

```
CONTINUOUS LOOP (every 60 seconds):

1. Read dependency-graph.json
2. Read sprint-status.yaml (which stories are "done")
3. For EACH incomplete story:
   - Check if ALL entries in its dependsOn array are "done"
   - If yes AND not already spawned â†’ spawn immediately
4. Track active spawns (session keys, start times)

UNLIMITED PARALLELISM â€” no cap, spawn ALL ready stories at once:
  - 1 story ready â†’ spawn 1
  - 10 stories ready â†’ spawn 10 simultaneously
  - 20 stories ready â†’ spawn 20 simultaneously
  - No waiting for "batches" to complete
```

**Per-Story Flow:**

**âš ï¸ CODE REVIEW CURRENTLY DISABLED (as of 2026-02-19)**

Stories go directly from dev â†’ done. Code review step is skipped to maximize factory throughput.

**Rationale:**
- Majority of reviews pass without changes (~80%+)
- Doubles subagent count per story (5-10 min overhead each)
- Phase 3 TEA testing suite catches issues more thoroughly
- Can re-enable once factory is mature and proven

**Current flow (single subagent per story):**

```
1. Spawn Amelia: dev-story
   â†’ Reads story + design_references (if design-assets.json exists)
   â†’ Uses Figma MCP to extract design specs (see [design-workflow.md](./design-workflow.md))
   â†’ Implements story with visual fidelity
   â†’ git pull, implement, git commit, git push to dev
   â†’ Status â†’ "done" (skipping review)

Story COMPLETE when dev work finishes
```

**When code review was enabled (historical):**

```
1. Spawn Amelia: dev-story
   â†’ Implement story
   â†’ git commit + push
   â†’ Write status = "done" + session_id to sprint-status.yaml
   â†’ Announce completion to PL
```

**Code review disabled (v3.3).** Dev â†’ done directly. No review subagent.
**To re-enable:** Add code-review spawn after dev-story, update status flow to review â†’ done.

**Subagent death handling:**
- If an Amelia session dies, PL detects (no completion announcement) and respawns
- Coding CLI fallback (Claude Code â†’ Codex) happens transparently within Amelia's execution (Claude Code primary as of 2026-02-18)
- Log failed attempts in daily memory notes with failure reason
- Increment version suffix on retry (e.g., `story-2.4-v1`, `story-2.4-v2`)

### Phase 3: Test

> â›” **HARD GATE â€” MANDATORY. CANNOT BE SKIPPED.**
> Phase 3 is NOT implemented as stories. Bob does NOT create test stories. Murat is NOT spawned by Amelia as a dev task. Project Lead directly orchestrates Phase 3 after ALL Phase 2 stories are `done`. You CANNOT set `phase = "qa"` or proceed to Phase 4 without `_bmad-output/test-artifacts/test-execution-report.md` existing and showing PASS.
>
> **A Phase 2 story named "Smoke Test", "Deploy", or "Production Build" is NOT Phase 3.** If that story completed, ignore it â€” Phase 3 still requires Murat's full test-generate â†’ E2E execution. This is what happened with Verdict: story 7.2 "Production Build, Deploy & Smoke Test" was marked done in Phase 2, PL skipped Phase 3, and the project shipped with zero real tests.

**Goal:** Ship a working, tested deployment. Pre-deploy catches build/lint issues cheaply. Post-deploy runs the TEA quality suite against the real deployed app.

---

#### Step 1: Pre-Deploy Gates

**Fast, cheap checks before deploying.** Failures batched â†’ Amelia remediates â†’ re-run gates.

```
Gate 1: Build Verification
  â†’ npm run build (or equivalent)
  â†’ Must produce clean build with zero errors

Gate 2: Lint & Type Checking
  â†’ npm run lint (ESLint/Biome)
  â†’ npx tsc --noEmit (TypeScript strict check)
  â†’ Zero errors required (warnings OK)

Gate 3: Security Scanning (Phase 2 â€” skip for now)
  â†’ npm audit --audit-level=high
  â†’ Dependency vulnerability check
  â†’ Known CVE scanning
```

**On failure:**
```
1. Batch ALL failures from Gates 1-3 into single remediation ticket
2. Spawn Amelia: fix-predeploy
   â†’ Input: Batched failure report (build errors, lint errors, type errors)
   â†’ Task: Fix all pre-deploy gate failures, commit, push to dev
3. Re-run Pre-Deploy Gates
4. Repeat until all gates pass (max 3 cycles, escalate to Kelly if stuck)
```

**Timeline:** 2-5 min per run. Remediation: 5-15 min per cycle.

---

#### Step 2: Deployment

**Deploy after pre-deploy gates pass. This is the ONE intentional deploy for the project.**

```
1. Check Vercel limit first (free tier: 100/day rolling):
   - Run: vercel --prod
   - If error "api-deployments-free-per-day" â†’ fall back to Firebase Hosting
   - Firebase fallback: firebase deploy --only hosting (no daily limit)
   - NOTE: Never use "push to dev triggers deploy" â€” git auto-deploy must be disabled

2. Verify deployment accessible:
   - Confirm live URL returns 200
   - Write both fields to `project-state.json` (dashboard reads these):
     ```bash
     python3 -c "
     import json; f='project-state.json'; d=json.load(open(f))
     d['qaUrl'] = 'DEPLOYED_URL'
     d['deployedUrl'] = 'DEPLOYED_URL'
     json.dump(d, open(f,'w'), indent=2)
     "
     ```
   - `qaUrl` = the URL used for QA testing (same as deployed URL)
   - `deployedUrl` = production URL â€” **persists in project-state.json forever** so the dashboard always shows it, even after shipping

3. If deploy fails:
   - Batch deployment errors â†’ Amelia fix â†’ redeploy
```

---

#### Step 3: Test (TEA Suite)

**Run the TEA quality suite against the DEPLOYED app.** Failures batched â†’ Amelia remediates â†’ redeploy â†’ re-run.

---

> ### ðŸš¨ E2E TESTING STANDARDS (NON-NEGOTIABLE)
>
> **No mocking in E2E tests. Real everything.**
>
> E2E tests must exercise the real, deployed app as a real user would. Mocking external services (Firebase Auth, APIs, etc.) in E2E tests defeats their purpose â€” it's the production environment (CSP headers, real OAuth flows, real network) that catches real bugs.
>
> **Rules Murat MUST follow when generating E2E tests:**
>
> 1. **No Firebase Auth mocking.** Do NOT stub `signInWithPopup`, `signInWithEmailAndPassword`, or any Firebase Auth method in E2E tests. Use real auth flows with a dedicated test account.
>
> 2. **Dedicated test account required.** If the project uses auth, Murat must check for a test credential file at `_bmad-output/test-artifacts/test-credentials.md`. If it doesn't exist, Murat outputs a blocker in `test-strategy.md` and halts â€” PL must obtain credentials before proceeding. PL notifies Kelly: "Blocked: need test account credentials for E2E auth flows."
>
> 3. **Console error assertions mandatory.** Every test file must include a `page.on('console', ...)` listener that captures browser console errors. Any CSP violation, uncaught JS error, or network failure detected in the console automatically fails the test.
>
> 4. **Tests run against deployed URL.** `playwright.config.ts` must set `baseURL` to `implementation.qaUrl` (the deployed app), not localhost. E2E tests do NOT spin up a dev server.
>
> 5. **Unit/integration tests may use mocks.** Only E2E is held to this standard. Unit and integration tests may mock freely.
>
> **Test credential file format** (`_bmad-output/test-artifacts/test-credentials.md`):
> ```
> ## Test Account
> Email: [dedicated test Gmail - NOT kelly@bloomtech.com]
> Password: [stored in env var TEST_ACCOUNT_PASSWORD]
> Provider: google / email / etc.
> Notes: This account is used exclusively for automated E2E testing.
> ```
>
> **Where to get credentials:** Kelly maintains a dedicated factory test Gmail. Ask Kelly via PL message: "Need test credentials for {projectId} E2E auth flows." Kelly will create the credentials file.

---

**Step 3a: Test Generation (one-time, Murat test-generate workflow)**

```
Murat: test-generate
  â†’ Input: PRD, architecture.md, acceptance criteria, codebase, tech stack
  â†’ Output:
    - _bmad-output/test-artifacts/test-strategy.md (design + coverage plan)
    - Playwright config (baseURL = deployed qaUrl, NOT localhost)
    - test helpers, fixtures scaffolded
    - Comprehensive E2E tests (user flows, auth, CRUD, navigation)
      âš ï¸  Auth flows: REAL credentials, no mocking (see E2E Testing Standards above)
      âš ï¸  Console error listener in every test file (CSP violations = auto-fail)
    - Accessibility checks (axe-core) included in E2E tests
  â†’ Duration: 25-45 min (combined design + scaffold + generate in one pass)
  â†’ BLOCKER: If project uses auth and test-credentials.md does not exist â†’ halt + notify Kelly
```

**Step 3b: Execution + NFR (parallel after test-generate completes)**

```
Parallel spawn:
  A. E2E Test Execution (against deployed app):
     â†’ Run Playwright tests against live URL (implementation.qaUrl)
     â†’ Reports pass/fail per test
     â†’ Screenshot evidence for failures
     â†’ Duration: 5-15 min
     â†’ Output: _bmad-output/test-artifacts/test-execution-report.md

  B. Murat: nfr workflow â€” NFR Assessment:
     â†’ Security: Auth vulnerabilities, XSS/CSRF, API exposure
     â†’ Performance: Load time, bundle size, database queries
     â†’ Accessibility: WCAG compliance (supplementary to E2E checks)
     â†’ Duration: 25-35 min
     â†’ Output: _bmad-output/test-artifacts/nfr-assessment-report.md

Wait for BOTH to complete before proceeding.
```

**Regression tests (brownfield only):**
```
If brownfield project (existing codebase):
  â†’ Run existing test suite to verify no regressions
  â†’ Any new failures are treated as blockers
```

---

#### Step 4: Remediation (Batched)

**ALL failures from Post-Deploy Verification batched â†’ Amelia â†’ redeploy â†’ re-run.**

```
1. Collect ALL failures:
   - E2E test failures (test-execution-report.md)
   - NFR issues (nfr-assessment-report.md)

2. Spawn Amelia: fix-postdeploy
   â†’ Input: Batched failure report from all TEA outputs
   â†’ Task: Fix all failures. For each:
     - Test failures â†’ Fix implementation code (not the tests)
     - NFR issues â†’ Fix security/performance/accessibility issues
   â†’ Commit + push to dev

3. Redeploy (Step 2)

4. Re-run Step 3b only (tests already generated â€” skip test-generate)
   â†’ Re-run E2E execution + NFR assessment in parallel
   â†’ Duration: 10-20 min (much faster)

5. Repeat until clean (max 3 cycles, escalate to Kelly if stuck)

6. **When Murat announces completion (pass or fail):**
   - Move Murat from `activeSubagents` â†’ `completedSubagents` in project-state.json
   - Murat exits after announcing â€” do NOT leave in active list
```

**Timeline:**
- Pre-Deploy Gates: 2-5 min
- Deployment: 2-5 min
- Test First Pass:
  - Murat test-generate (design + scaffold + generate): 25-45 min
  - E2E execution + NFR (parallel): 25-35 min
- **Total first pass: ~55-90 min**
- **Re-runs (execution only): 10-20 min** (tests already generated)
- Remediation per cycle: 15-30 min

**Key principle:** First pass generates tests once. Re-runs are cheap (just execution). Invest upfront, iterate fast.

### Phase 4: User QA

**When Phase 3 (Test) passes**, the app is already deployed (from Phase 3 Step 2). Notify Kelly, set pending-qa, then **hold** â€” the PL session must NOT exit until the operator ships or kills the project.

#### Stage 4.1: Notify Kelly + Set pending-qa

```javascript
sessions_send(
  sessionKey="agent:main",
  message="ðŸ§ª Project {projectName} passed automated testing. Ready for user QA: {qaUrl}\n\nDeployed at: {deployedUrl}"
)
```

Update projects-registry.json:
- **Set `state: "pending-qa"`** (was `in-progress`)
- Set `surfacedForQA: false` (Kelly will set to true after announcing)
- Ensure `implementation.qaUrl` is set

#### Stage 4.2: HOLD â€” Wait for Operator Signal

**DO NOT EXIT.** The PL session must stay alive (lock file held) so the project appears on the dashboard as an active session awaiting QA. Kelly will send a message when the operator makes a decision.

```
PL behavior: idle wait.
â†’ Reply to any incoming heartbeat with current status (project name, qaUrl, state=pending-qa).
â†’ Do NOT poll the registry in a loop. Just wait for a sessions_send message.
â†’ Acceptable wait: hours or days. Do not time out.
```

**Kelly's signal will be one of:**
- `"SHIP: {projectId}"` â†’ proceed to Stage 4.4 (Ship)
- `"FIX: {projectId} â€” {feedback}"` â†’ proceed to Stage 4.3 (Fix)
- `"PAUSE: {projectId}"` â†’ update registry `paused: true`, stay idle

#### Stage 4.3: Operator Testing â€” Fix Path

**Operator decides WHAT goes in. PL decides HOW to route it.**

> â›” **PL NEVER writes stories or creates story IDs itself.** Story creation is John's job. Dependency graphs are Bob's job. PL is a router â€” classify the feedback, pick the path, spawn the right agent. That's it.

```
1. Receive fix feedback from Kelly
   â†’ Example: "FIX: takeouttrap â€” Checkout flow confusing, auth broken on mobile"

2. Route each feedback item through the decision tree:

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Is it a GENUINE BUG?                                            â”‚
   â”‚ (feature was specified, it just doesn't work as described)      â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ YES â†’ Amelia: fix-qa-feedback                                   â”‚
   â”‚   Input: operator feedback verbatim + relevant story/ACs        â”‚
   â”‚   Output: fix committed, pushed to dev                          â”‚
   â”‚   No new stories. Amelia notes fix in sprint-status.yaml.       â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ NO â†’ Does it require architecture changes?                      â”‚
   â”‚   (new services, new data models, new integrations)             â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ YES â†’ Winston first: review-architecture-change                 â”‚
   â”‚   â†’ Then John: scope-qa-feedback (using Winston's output)       â”‚
   â”‚   â†’ Then Bob: update-dependency-graph                           â”‚
   â”‚   â†’ Then Phase 2 loop (Amelia)                                  â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ NO â†’ John: scope-qa-feedback (default story path)               â”‚
   â”‚   Input: operator feedback + prd.md + architecture.md           â”‚
   â”‚   Output: new story files + sprint-status.yaml entries          â”‚
   â”‚   â†’ Then Bob: update-dependency-graph                           â”‚
   â”‚   â†’ Then Phase 2 loop (Amelia)                                  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. After all fixes/stories complete:
   â†’ Re-run Phase 3 (Test): pre-deploy gates â†’ deploy â†’ TEA execution
   â†’ test-generate NOT re-run unless major new flows added â€” Murat reuses existing suite

4. Back to Stage 4.1 (re-notify Kelly, re-enter hold)
```

**When in doubt, use the Story Path (John).** Story creation is cheap. It keeps work visible in sprint-status.yaml, gives Amelia clear ACs, and makes the project auditable. Bug Path is the narrow exception.

#### Stage 4.4: Ship (on operator approval)

**Triggered by Kelly sending `"SHIP: {projectId}"`**

```bash
git checkout main && git merge dev && git push origin main
# CI/CD deploys to production from main
```

Update projects-registry.json:
- `state: "shipped"`
- `timeline.shippedAt: (now)`
- `implementation.deployedUrl: {productionUrl}`

Notify Kelly:
```javascript
sessions_send(
  sessionKey="agent:main",
  message="ðŸš€ {projectName} is live: {deployedUrl}"
)
```

**Then exit cleanly.** The session lock is released only after shipping is confirmed.

---

## Normal Mode Brownfield (BMAD Project)

**When:** Adding features to existing BMAD project (has `_bmad-output/` directory)

**NEW FEATURE ROUTING:**
- **Simple features** (well-defined, small scope) â†’ Direct to Phase 1 planning
- **Complex features** (architectural impact, scope uncertainty) â†’ **correct-course first**
  - Spawn John: correct-course to analyze impact, recommend approach
  - Sprint Change Proposal identifies needed changes (PRD, architecture, epics)
  - Then proceed to Phase 1 with clear plan

### Correct Course Order of Operations

**Rule: determine what changed, then sequence accordingly.**

| Change Type | Who goes first | Sequence |
|-------------|---------------|----------|
| Requirements changed (new/different features) | John | John (PRD edit) â†’ Sally (UX edit if needed) â†’ Winston (arch edit if needed) â†’ John (epics) â†’ Bob â†’ Amelia |
| Tech stack changed (same features, different how) | Winston | Winston (arch rewrite) â†’ John (epics update) â†’ Bob â†’ Amelia |
| Bug / missed implementation | Amelia directly | Amelia fixes â†’ no planning needed |
| Both requirements + tech changed | John | John (PRD edit) â†’ Winston (arch edit) â†’ John (epics) â†’ Bob â†’ Amelia |

**Key principle:** John always writes/updates epics *after* the architecture is settled â€” never before. Winston always works from an up-to-date PRD â€” never before John has clarified requirements.

**Skip agents who have nothing to change.** If UX isn't affected, don't spawn Sally. If architecture isn't affected, don't spawn Winston. Only touch what changed.

### Phase 1: Plan

```
0. Detect existing _bmad-output/ â†’ read existing artifacts
   â†’ Determine change type (requirements / tech / both) â†’ route per table above

1-3. John/Sally/Winston: Read existing PRD/UX/Architecture
     â†’ Update in EDIT mode ONLY if changes needed
     â†’ Skip if no changes
     â†’ Follow order-of-operations table above

4. John: create-epics-and-stories (ADD to existing epics.md)
   â†’ Continue numbering: Epic N+1, N+2...

5. John: check-implementation-readiness (for NEW features)
   â†’ Same PASS/NOT PASS logic as Greenfield (see above)
   â†’ Remediation loop targets only NEW artifacts (edit new epics, not existing ones)
   â†’ Repeat until PASS

6-7. Bob: Update sprint-planning + dependency-graph.json (add new stories)

8. Bob: create-story (ONE spawn â€” writes all NEW stories sequentially in one session)
```

### Phase 2-4: Same as Normal Mode Greenfield

---

## Normal Mode Brownfield (Non-BMAD Project)

**When:** Adding features to existing codebase without `_bmad-output/`

### Phase 1: Plan

```
0. document-project (FULL CODEBASE ANALYSIS â€” first time only)
   â†’ Output: _bmad-output/project-knowledge/index.md + parts/

1. generate-project-context (optional)
   â†’ Output: _bmad-output/project-context.md

2-8. Same as Normal Greenfield
   â†’ All personas read project-knowledge/ for context
   â†’ PRD includes "Modifications to Existing System" section
   â†’ Architecture is UPDATE document, not replacement
```

### Phase 2-4: Same as Normal Mode Greenfield

---

## State Management

### projects/projects-registry.json (Project Lead updates)

Update your project entry at key lifecycle transitions:

```bash
# Example: Mark project as in-progress
# Note: projectDir is relative to /Users/austenallred/clawd/projects/
jq '.projects |= map(
  if .id == "your-project-id" then
    .state = "in-progress" |
    .implementation.projectDir = "your-project-name" |
    .timeline.startedAt = (now|todate) |
    .timeline.lastUpdated = (now|todate)
  else . end
)' projects/projects-registry.json > tmp && mv tmp projects/projects-registry.json
```

**When to update:**
- Project start (`discovery` â†’ `in-progress`, set projectDir + startedAt)
- Test phase complete (`in-progress` â†’ `pending-qa`, set `implementation.qaUrl`, update `lastUpdated`, notify Kelly)
- Ship (`pending-qa` â†’ `shipped`, set deployedUrl + shippedAt) â€” only after receiving `"SHIP: {projectId}"` from Kelly
- Pause/resume (set `paused` + `pausedReason`)

### _bmad-output/implementation-artifacts/ (BMAD tracks)

**Story status:** `sprint-status.yaml`
- Which stories are done/in-progress/todo
- Updated by Bob and Amelia

**Dependencies:** `dependency-graph.json`
- Story dependency tree
- Created by Bob, read by PL for spawning logic

---

## Kelly Communication

**PL â†’ Kelly notifications:**
- Phase transitions (planning complete, build started, etc.)
- All stories complete â†’ entering test phase
- Test results (pass/fail)
- Ready for User QA (with qaUrl)
- Ship complete

**Kelly â†’ PL messages:**
- User QA feedback
- Pause/resume commands
- Status check pings (heartbeat safety net)

---

## Key Rules

1. **Dependency-graph.json is the authority** for story ordering â€” not artificial batches
2. **Spawn immediately** when dependencies satisfy â€” don't wait for groups
3. **Update registry at lifecycle transitions** â€” Kelly reads for monitoring
4. **One subagent per story** â€” dev-story only (code-review disabled as of v3.3)
5. **Detect dead subagents** â€” no completion = likely dead, respawn
6. **BMAD tracks story status** â€” sprint-status.yaml is source of truth
7. **All work on `dev` branch** â€” merge to `main` only at Ship
8. **Stay stateless** â€” terse replies only, all state in files, rotate session every 25 stories

## Context Discipline (v4.1 - 2026-02-19)

**PL is a stateless orchestrator. Session context must stay small.**

A 74-story project accumulates 280k+ tokens if PL narrates everything. This causes silent death at the 200k limit. The fix is behavioral â€” PL must externalize all state and keep replies minimal.

### Rules
- **Terse replies:** Max 1-2 lines per heartbeat (`âœ“ Spawned 10.7, 11.5` or `HEARTBEAT_OK`)
- **Never narrate history:** Don't summarize past waves. sprint-status.yaml has this.
- **Never quote sprint-status back:** Read â†’ act â†’ reply tersely. Don't copy file contents into context.
- **Subagent completions:** Update sprint-status.yaml status only. No elaboration.

### Voluntary Session Rotation (every 25 stories done)
At 25 stories complete, PL proactively archives its own session before hitting the 200k limit:

```bash
# Archive current session transcript
SESSION=$(ls ~/.openclaw/agents/project-lead/sessions/*.jsonl | grep -v ".lock\|.deleted\|.overflow" | head -1)
cp "$SESSION" "${SESSION}.overflow-archived-$(date +%Y%m%d-%H%M%S)"
echo '[]' > "$SESSION"
```

Then notify Kelly: `"PL session rotated at 25 stories. Resuming from sprint-status.yaml."`

**Why 25 stories:** At ~3k tokens/story spawn cycle, 25 stories = ~75k tokens. Well under the 200k limit even with heartbeat overhead. Fresh session picks up state from sprint-status.yaml instantly.

### Target Session Size
- Per-session token budget: **< 75k tokens**
- Trigger rotation at: **25 stories completed** (regardless of project size)
- Safety net: Kelly's "PL Context Overflow Guard" cron (every 30 min) catches any drift
